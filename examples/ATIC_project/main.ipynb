{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATIC Project\n",
    "Based on the repo \"Federated-Learning-Source\" we propose a model and dataset which can be integrated into this framework for financial forecasting and decision-making.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Since this project did not run without fixing some parts we present updated setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database\n",
    "As Data managing system Mongodb is used. This Database is only accessed by the global server and does not store any training or test data, instead handles saving and aggegration of model parameters, experiment hyperparamters and node task parameters.  \n",
    "To setup the Database follow these steps (MacOS):  \n",
    "\n",
    "1. We recommend brew for installation:  \n",
    "`brew tap mongodb/brew`  \n",
    "`brew install mongodb-community`  \n",
    "`brew services start mongodb-community`  \n",
    "Note: Might need a terminal restart for 'mongod' to work\n",
    "2. Setup database:  \n",
    "    1. Start MongoDB deamon (Note: choose where to save database locally):  \n",
    "    `mongod --port 27017 --dbpath /your/local/path --replSet rs0`\n",
    "    2. Open MongoDB shell and initialize replica set:  \n",
    "    `mongosh --port 27017`\n",
    "    3. In the MongoDB shell, run:  \n",
    "    `rs.initiate()`\n",
    "    4. Create admin user:  \n",
    "    `use admin`  \n",
    "    `db.createUser({\n",
    "       user: \"myUserAdmin\",\n",
    "       pwd: passwordPrompt(),\n",
    "       roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" }, \"readWriteAnyDatabase\" ]\n",
    "     })`\n",
    "    5. Stop demon:  \n",
    "    `CTRL+C`\n",
    "    6. Create a key file for authorization and save it:\n",
    "    `openssl rand -base64 741 > /your/local/path/mongo-keyfile`\n",
    "    `chmod 600 /your/local/path/mongo-keyfile`\n",
    "    7. Stop the daemon and restart with authentication (--fork ensures running the deamon in the background):  \n",
    "    `mongod --port 27017 --dbpath /your/local/path --replSet rs0 --auth --fork --logpath /your/local/path/mongod.log --keyFile /your/local/path/mongo-keyfile`\n",
    "    8. Open MongoDB shell again (Note: password needed now) and create database:  \n",
    "    `mongosh --port 27017 -u \"myUserAdmin\" -p --authenticationDatabase \"admin\"`\n",
    "    9. Create Database:  \n",
    "    `use federated_learning`  \n",
    "    `db.model.insert({})`  \n",
    "    `db.experiment.insert({})`  \n",
    "    `db.task.insert({})`\n",
    "    10. Add file `db_conf.key` in globalserver folder with content:  \n",
    "    {\"port\": \"27017\",\"host\":\"0.0.0.0\",\"user\": \"myUserAdmin\",\"password\": \"your password\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Environment\n",
    "As the global server runs locally even in distributed case we need to setup a virtual python environment. Use conda as it is very common:\n",
    "1. Init environemnt:  \n",
    "`conda create -n fl python=3.6`\n",
    "2. Activate env:  \n",
    "`conda activate fl`\n",
    "2. Navigation to root directory and run:  \n",
    "`pip install --requirement \"requirements.txt\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Server setup\n",
    "Add a file `envs.key` in root directory with content:  \n",
    "`{  \n",
    "    \"SERVER_ADDRESS\": \"0.0.0.0\",  \n",
    "    \"CLIENT_SECRET\": \"your_secret_here\",  \n",
    "    \"SERVER_PORT\": \"50000\",  \n",
    "    \"CLIENT_INTERFACE_PORT\": \"50001\",  \n",
    "    \"DATA_WRAPPER_URL\":\"None\"  \n",
    "}`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Mode\n",
    "For our project we do not actually need a real global mode as it is enough to just simulated the distributed setting on one local machine. Therefore no docker setup is needed and we can use the already implemented \"Testing\" Clasw which handles startup and shutdown of global and node servers already.  \n",
    "Note: If I am wrong and we need to run the node servers on docker it seems to have some bugs which I could not fully resolve yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "In the next part each code section provides a blueprint with comments to see what is needed to run the federated learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import os\n",
    "# TODO set working directory to your local path\n",
    "os.chdir('/path/federated-learning-source')\n",
    "import sys\n",
    "os.environ['STATIC_VARIABLES_FILE_PATH'] = \"globalserver/static_variables.json\"\n",
    "os.environ['PATH_TO_GLOBALSERVER'] = \"globalserver/api/\"\n",
    "sys.path.append(os.getcwd())\n",
    "import json\n",
    "\n",
    "# Importing the required Keras modules containing model and layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# TODO replace these imports with our own dataset\n",
    "from examples.dummy_example.utils import get_data,save_data_as_json,plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "The model must be defined in Keras and precompiled toghether with optimizer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kkbox_nn(parameters):\n",
    "    model = Sequential()\n",
    "    layers = 5\n",
    "    nodes = 16\n",
    "    lr = 0.01\n",
    "\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(nodes, activation=tf.nn.relu, input_shape=(2,)))\n",
    "        else:\n",
    "            model.add(Dense(nodes, activation=tf.nn.relu))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=lr, momentum=0.9),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[tf.metrics.AUC()])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "clients = [\"r1\",\"r0\"]\n",
    "\n",
    "\n",
    "setup_dict = {\n",
    "    \"model_function\": {\n",
    "        \"function\": kkbox_nn,\n",
    "        \"parameters\": {}\n",
    "    },\n",
    "    \"git_version\": 'e9339081b76ad3a89b1862bd38d8af26f0541f1c',\n",
    "    \"protocol\": 'NN',\n",
    "    \"model_name\": \"test_model\",\n",
    "    \"model_description\": \"this model is just to test the db\",\n",
    "    \"testing\": True,\n",
    "    \"training_config\": {\n",
    "        'epochs':  10,\n",
    "        'verbose': 2,\n",
    "        'batch_size': 100,\n",
    "        \"validation_steps\": 40,\n",
    "        # \"dataset\":'1',\n",
    "        \"test_steps\": 40,\n",
    "        \"steps_per_epoch\": 20,#int(14679/1000),\n",
    "        \"skmetrics\": [],\n",
    "        \"tfmetrics\": [\"AUC\", \"Accuracy\"],\n",
    "        \"differential_privacy\": {\"method\": 'before',},\n",
    "    },\n",
    "    \"rounds\": 30,\n",
    "    \"round\": [\"fetch_model\", \"train_model\", \"send_model\", \"send_training_loss\", \"send_test_loss\", \"aggregate\"],\n",
    "    \"final_round\": [\"fetch_model\",\"send_test_loss\", \"send_training_loss\"],\n",
    "    \"clients\": clients,\n",
    "    \"experiment_name\": \"kkbox\",\n",
    "    \"experiment_description\": f\"desc if nice experiment\",\n",
    "    \"stop_function\": None,\n",
    "    \"upkeep_function\": None,\n",
    "    \"preprocessing\": {\n",
    "        \"noise\": {\n",
    "            \"epsilon\": 10000,\n",
    "            \"delta\": 1\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server\n",
    "Next we start the Workers and the Global Server. The already implemented class Testing is very helpful as it handles the startup process for local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from globalserver.operator_.operator_class_db import Operator\n",
    "from testing.test_class import Testing\n",
    "\n",
    "TestSetup = Testing(clients, start_servers=True, clear_logs=True, clear_db=False, interface=False)\n",
    "operator = Operator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset \n",
    "The Dataset needs to be converted from numpy to json format. Therefore you need to create a folder datasets in root directory and adapt the functionality in save_data_as_json to our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just copied from dummy example \n",
    "# This is just and easy example dataset for binary classification \n",
    "# y = decision boundary\n",
    "# client_data_final = random subset of training data which can be accessed by individual clients\n",
    "training_data, client1_data_final,client2_data_final,y=get_data(exp=3)\n",
    "test_data, _,_,_=get_data(seed=10,exp=3)\n",
    "save_data_as_json(client1_data_final,client2_data_final,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "In this case the model is trained once where two node servers aggregated compared to both training individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={}\n",
    "for clients in [[\"r1\",\"r0\"],[\"r1\"],[\"r0\"]]:\n",
    "    setup_dict[\"clients\"]= clients\n",
    "    experiment_id,_=operator.define_and_start_experiment(setup_dict)\n",
    "    model=operator.get_compiled_model(protocol='NN', experiment_id=experiment_id)\n",
    "    models[f'{clients}']=model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "This is just and example of how to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clients,model in models.items():\n",
    "    model.evaluate(x=test_data[:,0:2],y=test_data[:,2], verbose=2)\n",
    "    y_pred=model.predict(x=test_data[:,:2])\n",
    "    test_data[:, 2]=[1 if y[0]>0.5 else 0 for y in y_pred]\n",
    "    # test_data[test_data[:,2]>0]: samples with label 1\n",
    "    # test_data[test_data[:,2]<1]: samples with label 0\n",
    "    # y: decision boundary\n",
    "    # plot_data: plots all three above datasets in one plot with different colors\n",
    "    plot_data([test_data[test_data[:,2]>0],test_data[test_data[:,2]<1],y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Servers\n",
    "Note: If Servers are not killed before starting a new session this can lead to issues. You can also check with `sudo lsof -i :27017` which processes are still running. This should list mongo as well as global and node server. With `kill -9 PID` you can end proccesses manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSetup.kill_servers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATIC Project\n",
    "Based on the repo \"Federated-Learning-Source\" we propose a model and dataset which can be integrated into this framework for financial forecasting and decision-making.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Since this project did not run without fixing some parts we present updated setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker\n",
    "To setup the Database as well as the python environment in Docker, we need to setup two seperate Docker container for each application and conncect them in a Docker network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database\n",
    "As Data managing system Mongodb is used. This Database is only accessed by the global server and does not store any training or test data, instead handles saving and aggegration of model parameters, experiment hyperparamters and node task parameters.  \n",
    "To setup the Database in Docker follow these steps:  \n",
    "\n",
    "1. Setup a Docker container for mongodb. In the terminal run:  \n",
    "    1. Create a docker volumn to store dataset\n",
    "    `docker volume create mongo_data`  \n",
    "    2. Create a docker network for communication between database and code\n",
    "    `docker network create fl_network`  \n",
    "    3. Download mongodb image for docker\n",
    "    `docker pull mongo`\n",
    "    4. Navigate to ATIC_project in terminal\n",
    "    5. Create a key file for authorization and save it:\n",
    "    `openssl rand -base64 741 > /your/local/path/mongo-keyfile`\n",
    "    `chmod 600 /your/local/path/mongo-keyfile` \n",
    "    6. Start docker container for mongodb\n",
    "    `docker run -d \\\n",
    "    --name mongodb_instance \\\n",
    "    --network fl_network \\\n",
    "    -v mongo_data:/data/db \\\n",
    "    -v $(pwd)/mongo-keyfile:/etc/mongo-keyfile \\\n",
    "    -e MONGO_INITDB_ROOT_USERNAME=admin \\\n",
    "    -e MONGO_INITDB_ROOT_PASSWORD=password \\\n",
    "    mongo:latest \\\n",
    "    mongod --replSet rs0 --bind_ip_all --keyFile /etc/mongo-keyfile --auth`\n",
    "\n",
    "2. Setup database:  \n",
    "    1. Connect to mongodb docker container:  \n",
    "    `docker exec -it mongodb_instance mongosh -u admin -p password`\n",
    "    2. In the MongoDB shell, run:  \n",
    "    `rs.initiate()`\n",
    "    3. Create Database:  \n",
    "    `use federated_learning`  \n",
    "    `db.model.insert({})`  \n",
    "    `db.experiment.insert({})`  \n",
    "    `db.task.insert({})`\n",
    "    10. Add file `db_conf.key` in globalserver folder with content:  \n",
    "    {\"port\": \"27017\",\"host\":\"mongodb_instance\",\"user\": \"admin\",\"password\": \"password\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App Docker\n",
    "Navigate to head direcrtory in terminal and run:\n",
    "1. Build docker image from Dockerfile:  \n",
    "`docker build -t my-python-jupyter .`\n",
    "2. Run docker container:  \n",
    "`docker run -d \\\n",
    "--name my-jupyter-app \\\n",
    "--network fl_network \\\n",
    "-p 8888:8888 \\\n",
    "-v \"$(pwd)\":/usr/src/app \\\n",
    "my-python-jupyter`\n",
    "3. Check for jupyter notebook adress: \n",
    "`docker logs my-jupyter-app`\n",
    "4. Copy the adress (should look something like this http://127.0.0.1:8888...) into browser to open notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Server setup\n",
    "Add a file `envs.key` in root directory with content:  \n",
    "`{  \n",
    "    \"SERVER_ADDRESS\": \"0.0.0.0\",  \n",
    "    \"CLIENT_SECRET\": \"your_secret_here\",  \n",
    "    \"SERVER_PORT\": \"50000\",  \n",
    "    \"CLIENT_INTERFACE_PORT\": \"50001\",  \n",
    "    \"DATA_WRAPPER_URL\":\"None\"  \n",
    "}`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "In the next part each code section provides a blueprint with comments to see what is needed to run the federated learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import os\n",
    "os.chdir('/usr/src/app')\n",
    "import sys\n",
    "os.environ['STATIC_VARIABLES_FILE_PATH'] = \"globalserver/static_variables.json\"\n",
    "os.environ['PATH_TO_GLOBALSERVER'] = \"globalserver/api/\"\n",
    "sys.path.append(os.getcwd())\n",
    "import json\n",
    "\n",
    "# Importing the required Keras modules containing model and layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# TODO replace these imports with our own dataset\n",
    "from examples.dummy_example.utils import get_data,save_data_as_json,plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "The model must be defined in Keras and precompiled toghether with optimizer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kkbox_nn(parameters):\n",
    "    model = Sequential()\n",
    "    layers = 5\n",
    "    nodes = 16\n",
    "    lr = 0.01\n",
    "\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(nodes, activation=tf.nn.relu, input_shape=(2,)))\n",
    "        else:\n",
    "            model.add(Dense(nodes, activation=tf.nn.relu))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=lr, momentum=0.9),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[tf.metrics.AUC()])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "clients = [\"r1\",\"r0\"]\n",
    "\n",
    "\n",
    "setup_dict = {\n",
    "    \"model_function\": {\n",
    "        \"function\": kkbox_nn,\n",
    "        \"parameters\": {}\n",
    "    },\n",
    "    \"git_version\": 'e9339081b76ad3a89b1862bd38d8af26f0541f1c',\n",
    "    \"protocol\": 'NN',\n",
    "    \"model_name\": \"test_model\",\n",
    "    \"model_description\": \"this model is just to test the db\",\n",
    "    \"testing\": True,\n",
    "    \"training_config\": {\n",
    "        'epochs':  10,\n",
    "        'verbose': 2,\n",
    "        'batch_size': 100,\n",
    "        \"validation_steps\": 40,\n",
    "        # \"dataset\":'1',\n",
    "        \"test_steps\": 40,\n",
    "        \"steps_per_epoch\": 20,#int(14679/1000),\n",
    "        \"skmetrics\": [],\n",
    "        \"tfmetrics\": [\"AUC\", \"Accuracy\"],\n",
    "        \"differential_privacy\": {\"method\": 'before',},\n",
    "    },\n",
    "    \"rounds\": 30,\n",
    "    \"round\": [\"fetch_model\", \"train_model\", \"send_model\", \"send_training_loss\", \"send_test_loss\", \"aggregate\"],\n",
    "    \"final_round\": [\"fetch_model\",\"send_test_loss\", \"send_training_loss\"],\n",
    "    \"clients\": clients,\n",
    "    \"experiment_name\": \"kkbox\",\n",
    "    \"experiment_description\": f\"desc if nice experiment\",\n",
    "    \"stop_function\": None,\n",
    "    \"upkeep_function\": None,\n",
    "    \"preprocessing\": {\n",
    "        \"noise\": {\n",
    "            \"epsilon\": 10000,\n",
    "            \"delta\": 1\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server\n",
    "Next we start the Workers and the Global Server. The already implemented class Testing is very helpful as it handles the startup process for local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from globalserver.operator_.operator_class_db import Operator\n",
    "from testing.test_class import Testing\n",
    "\n",
    "TestSetup = Testing(clients, start_servers=True, clear_logs=True, clear_db=False, interface=False)\n",
    "operator = Operator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset \n",
    "The Dataset needs to be converted from numpy to json format. Therefore you need to create a folder datasets in root directory and adapt the functionality in save_data_as_json to our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just copied from dummy example \n",
    "# This is just and easy example dataset for binary classification \n",
    "# y = decision boundary\n",
    "# client_data_final = random subset of training data which can be accessed by individual clients\n",
    "training_data, client1_data_final,client2_data_final,y=get_data(exp=3)\n",
    "test_data, _,_,_=get_data(seed=10,exp=3)\n",
    "save_data_as_json(client1_data_final,client2_data_final,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "In this case the model is trained once where two node servers aggregated compared to both training individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={}\n",
    "for clients in [[\"r1\",\"r0\"],[\"r1\"],[\"r0\"]]:\n",
    "    setup_dict[\"clients\"]= clients\n",
    "    experiment_id,_=operator.define_and_start_experiment(setup_dict)\n",
    "    model=operator.get_compiled_model(protocol='NN', experiment_id=experiment_id)\n",
    "    models[f'{clients}']=model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "This is just and example of how to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clients,model in models.items():\n",
    "    model.evaluate(x=test_data[:,0:2],y=test_data[:,2], verbose=2)\n",
    "    y_pred=model.predict(x=test_data[:,:2])\n",
    "    test_data[:, 2]=[1 if y[0]>0.5 else 0 for y in y_pred]\n",
    "    # test_data[test_data[:,2]>0]: samples with label 1\n",
    "    # test_data[test_data[:,2]<1]: samples with label 0\n",
    "    # y: decision boundary\n",
    "    # plot_data: plots all three above datasets in one plot with different colors\n",
    "    plot_data([test_data[test_data[:,2]>0],test_data[test_data[:,2]<1],y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Servers\n",
    "Note: If Servers are not killed before starting a new session this can lead to issues. You can also check with `sudo lsof -i :27017` which processes are still running. This should list mongo as well as global and node server. With `kill -9 PID` you can end proccesses manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSetup.kill_servers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
